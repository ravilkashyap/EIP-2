{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNyZv-Ec52ot"
   },
   "source": [
    "# **Import Libraries and modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3m3w1Cw49Zkt",
    "outputId": "02bfe65a-1fe6-4d45-9a44-4e98845d5a15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eso6UHE080D4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zByEi95J86RD"
   },
   "source": [
    "### Load pre-shuffled MNIST data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "7eRM0QWN83PV",
    "outputId": "29585520-8479-4892-cf76-f3506fda95de"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "4a4Be72j8-ZC",
    "outputId": "79cc3f14-1ec6-4e2e-8f1b-4b5d63357de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5deb32add8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkmprriw9AnZ"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2m4YS4E9CRh"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0Mn0vAYD9DvB",
    "outputId": "ea04f1fe-6106-42fb-9191-3c1f5b2adf0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG8JiXR39FHC"
   },
   "outputs": [],
   "source": [
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "fYlFRvKS9HMB",
    "outputId": "d87903db-6578-4e3e-eab1-41ea4d1ff379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fV6Zru8Ot7xG"
   },
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "osKqT73Q9JJB",
    "outputId": "dd412fce-1129-4de6-9177-d492512f773d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intern_eyecare/anaconda3/envs/Keras-FCN/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
      "  \n",
      "/home/intern_eyecare/anaconda3/envs/Keras-FCN/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/intern_eyecare/anaconda3/envs/Keras-FCN/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  if sys.path[0] == '':\n",
      "/home/intern_eyecare/anaconda3/envs/Keras-FCN/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
      "/home/intern_eyecare/anaconda3/envs/Keras-FCN/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
      "/home/intern_eyecare/anaconda3/envs/Keras-FCN/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    " \n",
    "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))   \n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "          \n",
    "\n",
    "    \n",
    "model.add(Convolution2D(10, 3, 3, activation='relu'))   \n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Convolution2D(10, 3, 3, activation='relu'))  \n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "          \n",
    "model.add(Convolution2D(10, 5))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "          \n",
    "\n",
    "          \n",
    "          \n",
    "          \n",
    "# model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "# # model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "# model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "# model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "# model.add(Convolution2D(10, 1, activation='relu'))\n",
    "# model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "# # model.add(keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "# model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
    "# model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
    "# model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "# # model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
    "# model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
    "# model.add(Convolution2D(10, 5))\n",
    "# model.add(Flatten())\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8pXYE1ot6mn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "TzdAYg1k9K7Z",
    "outputId": "46fc9f78-601b-4f29-e425-b2e0f25e9d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 26, 26, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 22, 22, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 22, 22, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 9, 9, 10)          2890      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 9, 9, 10)          40        \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 7, 7, 10)          910       \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 7, 7, 10)          40        \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 5, 5, 10)          910       \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 5, 5, 10)          40        \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 1, 1, 10)          2510      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 14,716\n",
      "Trainable params: 14,528\n",
      "Non-trainable params: 188\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKrB76jM7Bd2"
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_acc.h5', verbose=1, monitor='val_acc',save_best_only=True, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of Multiple GPU\n",
    "from keras.utils import multi_gpu_model\n",
    "try:\n",
    "    model = multi_gpu_model(model)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zp6SuGrL9M3h"
   },
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=adam,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAta augmentation only for Training set\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "train_generator = gen.flow(X_train, Y_train, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "29/29 [==============================] - 6s 219ms/step - loss: 0.7778 - acc: 0.7471 - val_loss: 3.3075 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55470, saving model to best_acc.h5\n",
      "Epoch 2/150\n",
      "29/29 [==============================] - 3s 115ms/step - loss: 0.2120 - acc: 0.9353 - val_loss: 0.3200 - val_acc: 0.9138\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.55470 to 0.91380, saving model to best_acc.h5\n",
      "Epoch 3/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.1610 - acc: 0.9499 - val_loss: 1.2400 - val_acc: 0.7208\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91380\n",
      "Epoch 4/150\n",
      "29/29 [==============================] - 4s 150ms/step - loss: 0.1200 - acc: 0.9636 - val_loss: 0.1941 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.91380 to 0.94420, saving model to best_acc.h5\n",
      "Epoch 5/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.1017 - acc: 0.9683 - val_loss: 0.0788 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.94420 to 0.97700, saving model to best_acc.h5\n",
      "Epoch 6/150\n",
      "29/29 [==============================] - 4s 127ms/step - loss: 0.0985 - acc: 0.9694 - val_loss: 0.1106 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97700\n",
      "Epoch 7/150\n",
      "29/29 [==============================] - 4s 147ms/step - loss: 0.0928 - acc: 0.9713 - val_loss: 0.1545 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.97700\n",
      "Epoch 8/150\n",
      "29/29 [==============================] - 5s 166ms/step - loss: 0.0845 - acc: 0.9731 - val_loss: 0.0659 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.97700 to 0.97850, saving model to best_acc.h5\n",
      "Epoch 9/150\n",
      "29/29 [==============================] - 5s 165ms/step - loss: 0.0858 - acc: 0.9740 - val_loss: 0.0959 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.97850\n",
      "Epoch 10/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0811 - acc: 0.9736 - val_loss: 0.1999 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97850\n",
      "Epoch 11/150\n",
      "29/29 [==============================] - 4s 155ms/step - loss: 0.0723 - acc: 0.9770 - val_loss: 0.0717 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.97850 to 0.97880, saving model to best_acc.h5\n",
      "Epoch 12/150\n",
      "29/29 [==============================] - 5s 166ms/step - loss: 0.0680 - acc: 0.9789 - val_loss: 0.2383 - val_acc: 0.9353\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.97880\n",
      "Epoch 13/150\n",
      "29/29 [==============================] - 5s 164ms/step - loss: 0.0751 - acc: 0.9771 - val_loss: 0.0614 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.97880 to 0.98260, saving model to best_acc.h5\n",
      "Epoch 14/150\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.0692 - acc: 0.9774 - val_loss: 0.0600 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.98260 to 0.98320, saving model to best_acc.h5\n",
      "Epoch 15/150\n",
      "29/29 [==============================] - 4s 144ms/step - loss: 0.0582 - acc: 0.9817 - val_loss: 0.0499 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98320 to 0.98490, saving model to best_acc.h5\n",
      "Epoch 16/150\n",
      "29/29 [==============================] - 5s 159ms/step - loss: 0.0582 - acc: 0.9832 - val_loss: 0.1639 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.98490\n",
      "Epoch 17/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0757 - acc: 0.9768 - val_loss: 0.1402 - val_acc: 0.9599\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.98490\n",
      "Epoch 18/150\n",
      "29/29 [==============================] - 4s 142ms/step - loss: 0.0609 - acc: 0.9807 - val_loss: 0.0564 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98490\n",
      "Epoch 19/150\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 0.0673 - acc: 0.9795 - val_loss: 0.1518 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98490\n",
      "Epoch 20/150\n",
      "29/29 [==============================] - 4s 136ms/step - loss: 0.0622 - acc: 0.9811 - val_loss: 0.0450 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.98490 to 0.98580, saving model to best_acc.h5\n",
      "Epoch 21/150\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.0544 - acc: 0.9834 - val_loss: 0.1432 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98580\n",
      "Epoch 22/150\n",
      "29/29 [==============================] - 4s 150ms/step - loss: 0.0562 - acc: 0.9824 - val_loss: 0.0458 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.98580 to 0.98620, saving model to best_acc.h5\n",
      "Epoch 23/150\n",
      "29/29 [==============================] - 4s 141ms/step - loss: 0.0517 - acc: 0.9843 - val_loss: 0.0570 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.98620\n",
      "Epoch 24/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0516 - acc: 0.9844 - val_loss: 0.0608 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.98620\n",
      "Epoch 25/150\n",
      "29/29 [==============================] - 5s 161ms/step - loss: 0.0548 - acc: 0.9830 - val_loss: 0.0562 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.98620\n",
      "Epoch 26/150\n",
      "29/29 [==============================] - 5s 170ms/step - loss: 0.0474 - acc: 0.9851 - val_loss: 0.0385 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.98620 to 0.98730, saving model to best_acc.h5\n",
      "Epoch 27/150\n",
      "29/29 [==============================] - 5s 166ms/step - loss: 0.0503 - acc: 0.9846 - val_loss: 0.0444 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.98730\n",
      "Epoch 28/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0482 - acc: 0.9855 - val_loss: 0.0429 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.98730\n",
      "Epoch 29/150\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.0466 - acc: 0.9862 - val_loss: 0.0356 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.98730 to 0.98880, saving model to best_acc.h5\n",
      "Epoch 30/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0511 - acc: 0.9840 - val_loss: 0.0878 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.98880\n",
      "Epoch 31/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0543 - acc: 0.9834 - val_loss: 0.0399 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.98880\n",
      "Epoch 32/150\n",
      "29/29 [==============================] - 4s 139ms/step - loss: 0.0472 - acc: 0.9846 - val_loss: 0.0492 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.98880\n",
      "Epoch 33/150\n",
      "29/29 [==============================] - 4s 145ms/step - loss: 0.0442 - acc: 0.9857 - val_loss: 0.0466 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.98880\n",
      "Epoch 34/150\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0415 - acc: 0.9873 - val_loss: 0.0556 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.98880\n",
      "Epoch 35/150\n",
      "29/29 [==============================] - 4s 154ms/step - loss: 0.0510 - acc: 0.9841 - val_loss: 0.0524 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.98880\n",
      "Epoch 36/150\n",
      "29/29 [==============================] - 4s 130ms/step - loss: 0.0455 - acc: 0.9866 - val_loss: 0.0459 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.98880\n",
      "Epoch 37/150\n",
      "29/29 [==============================] - 4s 140ms/step - loss: 0.0381 - acc: 0.9875 - val_loss: 0.0861 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.98880\n",
      "Epoch 38/150\n",
      "29/29 [==============================] - 4s 132ms/step - loss: 0.0451 - acc: 0.9859 - val_loss: 0.0451 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.98880\n",
      "Epoch 39/150\n",
      "29/29 [==============================] - 4s 139ms/step - loss: 0.0478 - acc: 0.9838 - val_loss: 0.0382 - val_acc: 0.9881\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.98880\n",
      "Epoch 40/150\n",
      "29/29 [==============================] - 4s 136ms/step - loss: 0.0462 - acc: 0.9857 - val_loss: 0.0557 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.98880\n",
      "Epoch 41/150\n",
      "29/29 [==============================] - 5s 159ms/step - loss: 0.0416 - acc: 0.9877 - val_loss: 0.0434 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.98880\n",
      "Epoch 42/150\n",
      "29/29 [==============================] - 4s 144ms/step - loss: 0.0391 - acc: 0.9875 - val_loss: 0.0393 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.98880\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 4s 133ms/step - loss: 0.0431 - acc: 0.9875 - val_loss: 0.0509 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.98880\n",
      "Epoch 44/150\n",
      "29/29 [==============================] - 5s 161ms/step - loss: 0.0421 - acc: 0.9868 - val_loss: 0.0632 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.98880\n",
      "Epoch 45/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0482 - acc: 0.9853 - val_loss: 0.0301 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.98880 to 0.99130, saving model to best_acc.h5\n",
      "Epoch 46/150\n",
      "29/29 [==============================] - 5s 161ms/step - loss: 0.0364 - acc: 0.9876 - val_loss: 0.0520 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.99130\n",
      "Epoch 47/150\n",
      "29/29 [==============================] - 5s 158ms/step - loss: 0.0443 - acc: 0.9858 - val_loss: 0.0578 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.99130\n",
      "Epoch 48/150\n",
      "29/29 [==============================] - 4s 148ms/step - loss: 0.0448 - acc: 0.9858 - val_loss: 0.0445 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.99130\n",
      "Epoch 49/150\n",
      "29/29 [==============================] - 4s 147ms/step - loss: 0.0386 - acc: 0.9883 - val_loss: 0.0405 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.99130\n",
      "Epoch 50/150\n",
      "29/29 [==============================] - 5s 160ms/step - loss: 0.0329 - acc: 0.9893 - val_loss: 0.0553 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.99130\n",
      "Epoch 51/150\n",
      "29/29 [==============================] - 4s 130ms/step - loss: 0.0352 - acc: 0.9885 - val_loss: 0.0313 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.99130\n",
      "Epoch 52/150\n",
      "29/29 [==============================] - 4s 144ms/step - loss: 0.0423 - acc: 0.9870 - val_loss: 0.0824 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.99130\n",
      "Epoch 53/150\n",
      "29/29 [==============================] - 4s 146ms/step - loss: 0.0448 - acc: 0.9862 - val_loss: 0.0375 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.99130\n",
      "Epoch 54/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0410 - acc: 0.9870 - val_loss: 0.0623 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.99130\n",
      "Epoch 55/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0416 - acc: 0.9869 - val_loss: 0.0738 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.99130\n",
      "Epoch 56/150\n",
      "29/29 [==============================] - 4s 128ms/step - loss: 0.0314 - acc: 0.9908 - val_loss: 0.0318 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.99130\n",
      "Epoch 57/150\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.0366 - acc: 0.9879 - val_loss: 0.0532 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.99130\n",
      "Epoch 58/150\n",
      "29/29 [==============================] - 5s 155ms/step - loss: 0.0381 - acc: 0.9876 - val_loss: 0.0450 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.99130\n",
      "Epoch 59/150\n",
      "29/29 [==============================] - 4s 128ms/step - loss: 0.0379 - acc: 0.9876 - val_loss: 0.0324 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.99130\n",
      "Epoch 60/150\n",
      "29/29 [==============================] - 4s 154ms/step - loss: 0.0355 - acc: 0.9894 - val_loss: 0.0379 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.99130\n",
      "Epoch 61/150\n",
      "29/29 [==============================] - 4s 145ms/step - loss: 0.0371 - acc: 0.9888 - val_loss: 0.0415 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.99130\n",
      "Epoch 62/150\n",
      "29/29 [==============================] - 5s 174ms/step - loss: 0.0342 - acc: 0.9894 - val_loss: 0.0292 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.99130\n",
      "Epoch 63/150\n",
      "29/29 [==============================] - 4s 152ms/step - loss: 0.0395 - acc: 0.9879 - val_loss: 0.0314 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.99130\n",
      "Epoch 64/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0381 - acc: 0.9888 - val_loss: 0.0575 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.99130\n",
      "Epoch 65/150\n",
      "29/29 [==============================] - 5s 170ms/step - loss: 0.0382 - acc: 0.9885 - val_loss: 0.0290 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.99130\n",
      "Epoch 66/150\n",
      "29/29 [==============================] - 5s 163ms/step - loss: 0.0340 - acc: 0.9883 - val_loss: 0.0392 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.99130\n",
      "Epoch 67/150\n",
      "29/29 [==============================] - 5s 170ms/step - loss: 0.0371 - acc: 0.9885 - val_loss: 0.0378 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.99130\n",
      "Epoch 68/150\n",
      "29/29 [==============================] - 5s 161ms/step - loss: 0.0375 - acc: 0.9871 - val_loss: 0.0302 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.99130\n",
      "Epoch 69/150\n",
      "29/29 [==============================] - 4s 149ms/step - loss: 0.0385 - acc: 0.9874 - val_loss: 0.0319 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.99130\n",
      "Epoch 70/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0405 - acc: 0.9870 - val_loss: 0.0722 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.99130\n",
      "Epoch 71/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0351 - acc: 0.9886 - val_loss: 0.0404 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.99130\n",
      "Epoch 72/150\n",
      "29/29 [==============================] - 4s 137ms/step - loss: 0.0396 - acc: 0.9890 - val_loss: 0.0275 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.99130\n",
      "Epoch 73/150\n",
      "29/29 [==============================] - 4s 152ms/step - loss: 0.0361 - acc: 0.9883 - val_loss: 0.0528 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.99130\n",
      "Epoch 74/150\n",
      "29/29 [==============================] - 4s 139ms/step - loss: 0.0318 - acc: 0.9897 - val_loss: 0.0641 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.99130\n",
      "Epoch 75/150\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.0300 - acc: 0.9910 - val_loss: 0.0281 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00075: val_acc improved from 0.99130 to 0.99170, saving model to best_acc.h5\n",
      "Epoch 76/150\n",
      "29/29 [==============================] - 4s 127ms/step - loss: 0.0345 - acc: 0.9902 - val_loss: 0.0433 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.99170\n",
      "Epoch 77/150\n",
      "29/29 [==============================] - 4s 147ms/step - loss: 0.0352 - acc: 0.9895 - val_loss: 0.0257 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00077: val_acc improved from 0.99170 to 0.99220, saving model to best_acc.h5\n",
      "Epoch 78/150\n",
      "29/29 [==============================] - 5s 160ms/step - loss: 0.0369 - acc: 0.9891 - val_loss: 0.0361 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.99220\n",
      "Epoch 79/150\n",
      "29/29 [==============================] - 4s 146ms/step - loss: 0.0300 - acc: 0.9895 - val_loss: 0.0326 - val_acc: 0.9901\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.99220\n",
      "Epoch 80/150\n",
      "29/29 [==============================] - 4s 149ms/step - loss: 0.0340 - acc: 0.9900 - val_loss: 0.0294 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.99220\n",
      "Epoch 81/150\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.0378 - acc: 0.9884 - val_loss: 0.0406 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.99220\n",
      "Epoch 82/150\n",
      "29/29 [==============================] - 5s 158ms/step - loss: 0.0362 - acc: 0.9888 - val_loss: 0.0473 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.99220\n",
      "Epoch 83/150\n",
      "29/29 [==============================] - 4s 146ms/step - loss: 0.0325 - acc: 0.9899 - val_loss: 0.0340 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.99220\n",
      "Epoch 84/150\n",
      "29/29 [==============================] - 4s 148ms/step - loss: 0.0316 - acc: 0.9898 - val_loss: 0.0287 - val_acc: 0.9915\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.99220\n",
      "Epoch 85/150\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0270 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.99220\n",
      "Epoch 86/150\n",
      "29/29 [==============================] - 5s 159ms/step - loss: 0.0358 - acc: 0.9883 - val_loss: 0.0449 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.99220\n",
      "Epoch 87/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 4s 154ms/step - loss: 0.0358 - acc: 0.9884 - val_loss: 0.0223 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00087: val_acc improved from 0.99220 to 0.99300, saving model to best_acc.h5\n",
      "Epoch 88/150\n",
      "29/29 [==============================] - 5s 161ms/step - loss: 0.0318 - acc: 0.9898 - val_loss: 0.0329 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.99300\n",
      "Epoch 89/150\n",
      "29/29 [==============================] - 4s 144ms/step - loss: 0.0296 - acc: 0.9914 - val_loss: 0.0347 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.99300\n",
      "Epoch 90/150\n",
      "29/29 [==============================] - 5s 162ms/step - loss: 0.0315 - acc: 0.9898 - val_loss: 0.0454 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.99300\n",
      "Epoch 91/150\n",
      "29/29 [==============================] - 4s 152ms/step - loss: 0.0351 - acc: 0.9884 - val_loss: 0.0337 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.99300\n",
      "Epoch 92/150\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.0316 - acc: 0.9897 - val_loss: 0.0375 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.99300\n",
      "Epoch 93/150\n",
      "29/29 [==============================] - 4s 134ms/step - loss: 0.0416 - acc: 0.9874 - val_loss: 0.0378 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.99300\n",
      "Epoch 94/150\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.0326 - acc: 0.9888 - val_loss: 0.0319 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.99300\n",
      "Epoch 95/150\n",
      "29/29 [==============================] - 5s 160ms/step - loss: 0.0331 - acc: 0.9901 - val_loss: 0.0241 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.99300\n",
      "Epoch 96/150\n",
      "29/29 [==============================] - 4s 155ms/step - loss: 0.0360 - acc: 0.9888 - val_loss: 0.0314 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.99300\n",
      "Epoch 97/150\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0284 - acc: 0.9909 - val_loss: 0.0255 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.99300\n",
      "Epoch 98/150\n",
      "29/29 [==============================] - 5s 161ms/step - loss: 0.0312 - acc: 0.9902 - val_loss: 0.0279 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.99300\n",
      "Epoch 99/150\n",
      "29/29 [==============================] - 4s 152ms/step - loss: 0.0274 - acc: 0.9914 - val_loss: 0.0308 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.99300\n",
      "Epoch 100/150\n",
      "29/29 [==============================] - 4s 153ms/step - loss: 0.0297 - acc: 0.9911 - val_loss: 0.0272 - val_acc: 0.9919\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.99300\n",
      "Epoch 101/150\n",
      "29/29 [==============================] - 4s 142ms/step - loss: 0.0322 - acc: 0.9895 - val_loss: 0.1060 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.99300\n",
      "Epoch 102/150\n",
      "29/29 [==============================] - 4s 146ms/step - loss: 0.0367 - acc: 0.9880 - val_loss: 0.0593 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.99300\n",
      "Epoch 103/150\n",
      "29/29 [==============================] - 5s 161ms/step - loss: 0.0290 - acc: 0.9910 - val_loss: 0.0263 - val_acc: 0.9915\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.99300\n",
      "Epoch 104/150\n",
      "29/29 [==============================] - 4s 141ms/step - loss: 0.0342 - acc: 0.9895 - val_loss: 0.0552 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.99300\n",
      "Epoch 105/150\n",
      "29/29 [==============================] - 4s 140ms/step - loss: 0.0320 - acc: 0.9896 - val_loss: 0.0703 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.99300\n",
      "Epoch 106/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0333 - acc: 0.9897 - val_loss: 0.0510 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.99300\n",
      "Epoch 107/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0305 - acc: 0.9901 - val_loss: 0.0380 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.99300\n",
      "Epoch 108/150\n",
      "29/29 [==============================] - 4s 140ms/step - loss: 0.0307 - acc: 0.9908 - val_loss: 0.0452 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.99300\n",
      "Epoch 109/150\n",
      "29/29 [==============================] - 5s 159ms/step - loss: 0.0326 - acc: 0.9899 - val_loss: 0.0479 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.99300\n",
      "Epoch 110/150\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0297 - acc: 0.9906 - val_loss: 0.0313 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.99300\n",
      "Epoch 111/150\n",
      "29/29 [==============================] - 4s 131ms/step - loss: 0.0260 - acc: 0.9919 - val_loss: 0.0280 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.99300\n",
      "Epoch 112/150\n",
      "29/29 [==============================] - 4s 153ms/step - loss: 0.0270 - acc: 0.9913 - val_loss: 0.0292 - val_acc: 0.9911\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.99300\n",
      "Epoch 113/150\n",
      "29/29 [==============================] - 5s 160ms/step - loss: 0.0306 - acc: 0.9897 - val_loss: 0.0334 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.99300\n",
      "Epoch 114/150\n",
      "29/29 [==============================] - 4s 147ms/step - loss: 0.0281 - acc: 0.9910 - val_loss: 0.0302 - val_acc: 0.9911\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.99300\n",
      "Epoch 115/150\n",
      "29/29 [==============================] - 4s 154ms/step - loss: 0.0268 - acc: 0.9916 - val_loss: 0.0311 - val_acc: 0.9901\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.99300\n",
      "Epoch 116/150\n",
      "29/29 [==============================] - 4s 148ms/step - loss: 0.0333 - acc: 0.9890 - val_loss: 0.0406 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.99300\n",
      "Epoch 117/150\n",
      "29/29 [==============================] - 5s 160ms/step - loss: 0.0302 - acc: 0.9899 - val_loss: 0.0244 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.99300\n",
      "Epoch 118/150\n",
      "29/29 [==============================] - 4s 142ms/step - loss: 0.0265 - acc: 0.9910 - val_loss: 0.0331 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.99300\n",
      "Epoch 119/150\n",
      "29/29 [==============================] - 4s 149ms/step - loss: 0.0339 - acc: 0.9896 - val_loss: 0.0380 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.99300\n",
      "Epoch 120/150\n",
      "29/29 [==============================] - 4s 125ms/step - loss: 0.0267 - acc: 0.9921 - val_loss: 0.0263 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.99300\n",
      "Epoch 121/150\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 0.0353 - acc: 0.9885 - val_loss: 0.0301 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.99300\n",
      "Epoch 122/150\n",
      "29/29 [==============================] - 4s 140ms/step - loss: 0.0342 - acc: 0.9894 - val_loss: 0.0332 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.99300\n",
      "Epoch 123/150\n",
      "29/29 [==============================] - 5s 169ms/step - loss: 0.0308 - acc: 0.9905 - val_loss: 0.0280 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.99300\n",
      "Epoch 124/150\n",
      "29/29 [==============================] - 5s 163ms/step - loss: 0.0332 - acc: 0.9901 - val_loss: 0.0293 - val_acc: 0.9915\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.99300\n",
      "Epoch 125/150\n",
      "29/29 [==============================] - 4s 147ms/step - loss: 0.0320 - acc: 0.9905 - val_loss: 0.0286 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.99300\n",
      "Epoch 126/150\n",
      "29/29 [==============================] - 4s 144ms/step - loss: 0.0272 - acc: 0.9915 - val_loss: 0.0300 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.99300\n",
      "Epoch 127/150\n",
      "29/29 [==============================] - 4s 150ms/step - loss: 0.0288 - acc: 0.9914 - val_loss: 0.0264 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.99300\n",
      "Epoch 128/150\n",
      "29/29 [==============================] - 4s 136ms/step - loss: 0.0260 - acc: 0.9919 - val_loss: 0.0221 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.99300 to 0.99300, saving model to best_acc.h5\n",
      "Epoch 129/150\n",
      "29/29 [==============================] - 4s 154ms/step - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0279 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.99300\n",
      "Epoch 130/150\n",
      "29/29 [==============================] - 5s 166ms/step - loss: 0.0258 - acc: 0.9916 - val_loss: 0.0297 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.99300\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 4s 140ms/step - loss: 0.0264 - acc: 0.9919 - val_loss: 0.0250 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.99300\n",
      "Epoch 132/150\n",
      "29/29 [==============================] - 4s 137ms/step - loss: 0.0245 - acc: 0.9925 - val_loss: 0.0433 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.99300\n",
      "Epoch 133/150\n",
      "29/29 [==============================] - 4s 143ms/step - loss: 0.0381 - acc: 0.9890 - val_loss: 0.0480 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.99300\n",
      "Epoch 134/150\n",
      "29/29 [==============================] - 4s 152ms/step - loss: 0.0291 - acc: 0.9906 - val_loss: 0.0519 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.99300\n",
      "Epoch 135/150\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0310 - acc: 0.9910 - val_loss: 0.0333 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.99300\n",
      "Epoch 136/150\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.0262 - acc: 0.9914 - val_loss: 0.0294 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.99300\n",
      "Epoch 137/150\n",
      "29/29 [==============================] - 4s 149ms/step - loss: 0.0239 - acc: 0.9923 - val_loss: 0.0517 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.99300\n",
      "Epoch 138/150\n",
      "29/29 [==============================] - 4s 145ms/step - loss: 0.0299 - acc: 0.9911 - val_loss: 0.0321 - val_acc: 0.9901\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.99300\n",
      "Epoch 139/150\n",
      "29/29 [==============================] - 4s 151ms/step - loss: 0.0253 - acc: 0.9923 - val_loss: 0.0355 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.99300\n",
      "Epoch 140/150\n",
      "29/29 [==============================] - 4s 130ms/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.0349 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.99300\n",
      "Epoch 141/150\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.0293 - acc: 0.9908 - val_loss: 0.0428 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.99300\n",
      "Epoch 142/150\n",
      "29/29 [==============================] - 3s 114ms/step - loss: 0.0248 - acc: 0.9924 - val_loss: 0.0273 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.99300\n",
      "Epoch 143/150\n",
      "29/29 [==============================] - 4s 122ms/step - loss: 0.0255 - acc: 0.9916 - val_loss: 0.0449 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.99300\n",
      "Epoch 144/150\n",
      "29/29 [==============================] - 4s 143ms/step - loss: 0.0281 - acc: 0.9910 - val_loss: 0.0279 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.99300\n",
      "Epoch 145/150\n",
      "29/29 [==============================] - 4s 121ms/step - loss: 0.0281 - acc: 0.9916 - val_loss: 0.0370 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.99300\n",
      "Epoch 146/150\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 0.0231 - acc: 0.9924 - val_loss: 0.0502 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.99300\n",
      "Epoch 147/150\n",
      "29/29 [==============================] - 4s 124ms/step - loss: 0.0231 - acc: 0.9923 - val_loss: 0.0264 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.99300\n",
      "Epoch 148/150\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.0244 - acc: 0.9932 - val_loss: 0.0392 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.99300\n",
      "Epoch 149/150\n",
      "29/29 [==============================] - 4s 138ms/step - loss: 0.0289 - acc: 0.9915 - val_loss: 0.0292 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.99300\n",
      "Epoch 150/150\n",
      "29/29 [==============================] - 4s 131ms/step - loss: 0.0243 - acc: 0.9927 - val_loss: 0.0348 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.99300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c351b6550>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use checkpoint to save the best model\n",
    "model.fit_generator(train_generator, steps_per_epoch=60000//2048, epochs=150, \n",
    "                    validation_data=(X_test,Y_test),callbacks=[checkpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "29/29 [==============================] - 4s 124ms/step - loss: 0.0291 - acc: 0.9907 - val_loss: 0.0262 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c34315048>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Val accuracy has reached 99.3 %\n",
    "#Use checkpoint to save the best model\n",
    "model.fit_generator(train_generator, steps_per_epoch=60000//2048, epochs=1, \n",
    "                    validation_data=(X_test,Y_test),callbacks=[checkpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtsH-lLk-eLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02616235318047693, 0.992]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mkX8JMv79q9r",
    "outputId": "98ffad90-5dd4-4402-e3a2-025e879b9617"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCWoJkwE9suh"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07lVATu0Azoj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "Ym7iCFBm9uBs",
    "outputId": "2838b78d-ffa1-4970-992c-228723e83f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.02517803e-11 4.22035651e-09 2.46206037e-05 6.19036244e-09\n",
      "  8.56240565e-11 7.50092488e-11 9.87527253e-14 9.99975324e-01\n",
      "  4.67228027e-11 3.69689213e-09]\n",
      " [4.12241121e-07 3.45949225e-08 9.99998927e-01 4.08514111e-09\n",
      "  4.43399806e-09 5.08103488e-13 4.29952195e-07 1.79820105e-08\n",
      "  6.81976644e-08 7.28693266e-08]\n",
      " [6.29280228e-09 9.99950647e-01 1.68751143e-07 2.63259814e-09\n",
      "  1.27349431e-05 1.37815208e-08 8.55515481e-08 3.63423351e-05\n",
      "  2.06426716e-08 2.03706421e-08]\n",
      " [9.96043801e-01 7.03062998e-12 1.48143647e-06 2.80312307e-08\n",
      "  1.37328220e-06 5.73993873e-07 3.87056335e-03 3.16781579e-09\n",
      "  4.07103926e-06 7.80415066e-05]\n",
      " [4.03476896e-09 1.93461158e-09 2.70810544e-11 3.30161029e-13\n",
      "  9.99981880e-01 1.95418005e-11 1.44340255e-08 1.98352157e-09\n",
      "  7.99578714e-09 1.81598080e-05]\n",
      " [1.93892671e-08 9.99552548e-01 3.98080253e-08 7.80786269e-10\n",
      "  1.80774572e-04 2.58185402e-08 3.50951659e-08 2.63247028e-04\n",
      "  1.82445721e-08 3.35232880e-06]\n",
      " [2.91067880e-18 8.33634661e-10 2.84290109e-14 1.24738224e-17\n",
      "  9.99997973e-01 1.56934029e-14 4.11579069e-15 1.53805502e-08\n",
      "  3.23054639e-11 2.02591423e-06]\n",
      " [1.09663661e-05 3.04853597e-06 6.84723709e-05 2.66140834e-08\n",
      "  6.80081686e-03 6.10071584e-06 1.05186878e-07 1.23285528e-04\n",
      "  3.42064704e-05 9.92953002e-01]\n",
      " [2.11288608e-07 2.80862319e-14 5.73988412e-10 3.03649585e-08\n",
      "  5.08674489e-08 2.82565624e-01 7.17070878e-01 2.50203608e-11\n",
      "  3.63205530e-04 5.01238073e-09]]\n",
      "[7 2 1 0 4 1 4 9 5]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:9])\n",
    "print(y_test[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TloMkb3NBOMa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CT--y98_dr2T"
   },
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GY4Upv4dsUR"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-02e7f701557c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filter %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mplot_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mvis_img_in_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-02e7f701557c>\u001b[0m in \u001b[0;36mvis_img_in_filter\u001b[0;34m(img, layer_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mplot_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input image and %s filters'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAAKvCAYAAABzr+mpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGORJREFUeJzt3VGI5ed93vHnZ6lqqOvEpdpAkJTYoXKdxRTsLq5LoHGwW2RdSDcmSGDSFGGRtE4vEgouLm5QrurQGgJqE9EaN4HYUXKRLEFB0NTGxUSu1thxLBmVreJGi0y9SVzfmNgWfXsxkzBdPas5u/ufWe/O5wMDc855Nfu+e2Z/fHXmnDmz1goAAPD/e9X13gAAAHwnEsoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAxaGhPDMfmZmvzswXL3P7zMwvzsz5mfnCzLxl+20CsCtzG2Abuzyi/NEk97zC7e9Kcvf+x8NJ/sO1bwuAa/DRmNsA1+zQUF5rfSrJn73CkvuT/Mra81SS187M9221QQCujLkNsI1bN/gadyR54cDlC/vXfeXShTPzcPYevcirX/3qv/vGN75xgz8e4Hh99rOf/ZO11qnrvY9rsNPcNrOBm8XVzu0tQnnKdfV9sddajyV5LEnOnDmzzp07t8EfD3C8ZuZ/Xe89XKOd5raZDdwsrnZub/FbLy4kuevA5TuTvLjB1wXgaJjbADvYIpTPJvnx/VdRvy3J19daL3vaBQDfMcxtgB0c+tSLmflYkrcnuX1mLiT510n+SpKstX4pyRNJ7k1yPsk3kvyTo9osAIcztwG2cWgor7UePOT2leSfbbYjAK6JuQ2wDe/MBwAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBip1CemXtm5rmZOT8z7y+3f//MfGJmPjczX5iZe7ffKgC7MLMBtnFoKM/MLUkeTfKuJKeTPDgzpy9Z9q+SPL7WenOSB5L8+603CsDhzGyA7ezyiPJbk5xfaz2/1vpWko8nuf+SNSvJd+9//j1JXtxuiwBcATMbYCO7hPIdSV44cPnC/nUH/VyS98zMhSRPJPnp9oVm5uGZOTcz5y5evHgV2wXgEGY2wEZ2CeUp161LLj+Y5KNrrTuT3JvkV2fmZV97rfXYWuvMWuvMqVOnrny3ABzGzAbYyC6hfCHJXQcu35mX/5juoSSPJ8la6/eTfFeS27fYIABXxMwG2Mguofx0krtn5vUzc1v2Xvhx9pI1f5zkHUkyMz+UvaHr53QAx8/MBtjIoaG81nopyfuSPJnkS9l7pfQzM/PIzNy3v+xnk7x3Zv4gyceS/MRa69If9QFwxMxsgO3cusuitdYT2XvBx8HrPnjg82eT/PC2WwPgapjZANvwznwAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFDsFMozc8/MPDcz52fm/ZdZ82Mz8+zMPDMzv7btNgHYlZkNsI1bD1swM7ckeTTJP0xyIcnTM3N2rfXsgTV3J/mXSX54rfW1mfneo9owAJdnZgNsZ5dHlN+a5Pxa6/m11reSfDzJ/ZeseW+SR9daX0uStdZXt90mADsyswE2skso35HkhQOXL+xfd9AbkrxhZj49M0/NzD3tC83MwzNzbmbOXbx48ep2DMArMbMBNrJLKE+5bl1y+dYkdyd5e5IHk/zHmXnty/6jtR5ba51Za505derUle4VgMOZ2QAb2SWULyS568DlO5O8WNb89lrr22utP0ryXPaGMADHy8wG2Mguofx0krtn5vUzc1uSB5KcvWTNbyX50SSZmduz92O957fcKAA7MbMBNnJoKK+1XkryviRPJvlSksfXWs/MzCMzc9/+sieT/OnMPJvkE0n+xVrrT49q0wB0ZjbAdmatS5+6djzOnDmzzp07d13+bIBrMTOfXWudud77OE5mNnAju9q57Z35AACgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFAIZQAAKIQyAAAUQhkAAAqhDAAAhVAGAIBCKAMAQCGUAQCgEMoAAFDsFMozc8/MPDcz52fm/a+w7t0zs2bmzHZbBOBKmNkA2zg0lGfmliSPJnlXktNJHpyZ02Xda5L88ySf2XqTAOzGzAbYzi6PKL81yfm11vNrrW8l+XiS+8u6n0/yoSR/vuH+ALgyZjbARnYJ5TuSvHDg8oX96/7SzLw5yV1rrd95pS80Mw/PzLmZOXfx4sUr3iwAhzKzATaySyhPuW795Y0zr0ry4SQ/e9gXWms9ttY6s9Y6c+rUqd13CcCuzGyAjewSyheS3HXg8p1JXjxw+TVJ3pTkkzPz5SRvS3LWi0MArgszG2Aju4Ty00nunpnXz8xtSR5IcvYvblxrfX2tdfta63VrrdcleSrJfWutc0eyYwBeiZkNsJFDQ3mt9VKS9yV5MsmXkjy+1npmZh6ZmfuOeoMA7M7MBtjOrbssWms9keSJS6774GXWvv3atwXA1TKzAbbhnfkAAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAACFUAYAgEIoAwBAIZQBAKDYKZRn5p6ZeW5mzs/M+8vtPzMzz87MF2bm92bmB7bfKgC7MLMBtnFoKM/MLUkeTfKuJKeTPDgzpy9Z9rkkZ9ZafyfJbyb50NYbBeBwZjbAdnZ5RPmtSc6vtZ5fa30ryceT3H9wwVrrE2utb+xffCrJndtuE4AdmdkAG9kllO9I8sKByxf2r7uch5L8brthZh6emXMzc+7ixYu77xKAXZnZABvZJZSnXLfqwpn3JDmT5Bfa7Wutx9ZaZ9ZaZ06dOrX7LgHYlZkNsJFbd1hzIcldBy7fmeTFSxfNzDuTfCDJj6y1vrnN9gC4QmY2wEZ2eUT56SR3z8zrZ+a2JA8kOXtwwcy8OckvJ7lvrfXV7bcJwI7MbICNHBrKa62XkrwvyZNJvpTk8bXWMzPzyMzct7/sF5L89SS/MTOfn5mzl/lyABwhMxtgO7s89SJrrSeSPHHJdR888Pk7N94XAFfJzAbYhnfmAwCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAEAhlAEAoBDKAABQCGUAACiEMgAAFEIZAAAKoQwAAIVQBgCAQigDAECxUyjPzD0z89zMnJ+Z95fb/+rM/Pr+7Z+ZmddtvVEAdmNmA2zj0FCemVuSPJrkXUlOJ3lwZk5fsuyhJF9ba/2tJB9O8m+23igAhzOzAbazyyPKb01yfq31/FrrW0k+nuT+S9bcn+Q/73/+m0neMTOz3TYB2JGZDbCRW3dYc0eSFw5cvpDk711uzVrrpZn5epK/meRPDi6amYeTPLx/8Zsz88Wr2fQN7PZc8ndyAjjzyXDSzvy3r/cGXoGZvZ2T9n2dOPNJcRLPfFVze5dQbo8yrKtYk7XWY0keS5KZObfWOrPDn3/TcOaTwZlvfjNz7nrv4RWY2Rtx5pPBmU+Gq53buzz14kKSuw5cvjPJi5dbMzO3JvmeJH92NRsC4JqY2QAb2SWUn05y98y8fmZuS/JAkrOXrDmb5B/vf/7uJP91rfWyRycAOHJmNsBGDn3qxf7z196X5MkktyT5yFrrmZl5JMm5tdbZJP8pya/OzPnsPSrxwA5/9mPXsO8blTOfDM588/uOPa+ZvSlnPhmc+WS4qjOPBxEAAODlvDMfAAAUQhkAAIojD+WT+FaqO5z5Z2bm2Zn5wsz83sz8wPXY55YOO/OBde+emTUzN/SvpdnlvDPzY/v38zMz82vHvcet7fB9/f0z84mZ+dz+9/a912OfW5qZj8zMVy/3+4Nnzy/u/518YWbectx73JqZbWZfsu6mmNmJuX0S5vaRzOy11pF9ZO+FJP8zyQ8muS3JHyQ5fcmaf5rkl/Y/fyDJrx/lno76Y8cz/2iSv7b/+U+dhDPvr3tNkk8leSrJmeu97yO+j+9O8rkkf2P/8vde730fw5kfS/JT+5+fTvLl673vDc79D5K8JckXL3P7vUl+N3u/l/htST5zvfd8DPezmX0Czry/7qaY2VdwP5vbN/jcPoqZfdSPKJ/Et1I99MxrrU+stb6xf/Gp7P2e0xvZLvdzkvx8kg8l+fPj3NwR2OW8703y6Frra0my1vrqMe9xa7uceSX57v3Pvycv/929N5y11qfyyr9f+P4kv7L2PJXktTPzfcezuyNhZpvZB90sMzsxt0/E3D6KmX3UodzeSvWOy61Za72U5C/eSvVGtcuZD3ooe/93cyM79Mwz8+Ykd621fuc4N3ZEdrmP35DkDTPz6Zl5ambuObbdHY1dzvxzSd4zMxeSPJHkp49na9fVlf57/05nZpvZSW66mZ2Y24m5nVzFzN7lLayvxWZvpXoD2fk8M/OeJGeS/MiR7ujoveKZZ+ZVST6c5CeOa0NHbJf7+Nbs/Rjv7dl79Om/zcyb1lr/54j3dlR2OfODST661vq3M/P3s/d7et+01vq/R7+96+Ykzq+TeOa9hWb2jczc3nPS5/YVz6+jfkT5JL6V6i5nzsy8M8kHkty31vrmMe3tqBx25tckeVOST87Ml7P3vKCzN/CLQ3b9vv7ttda311p/lOS57A3gG9UuZ34oyeNJstb6/STfleT2Y9nd9bPTv/cbiJltZic338xOzO3E3E6uYmYfdSifxLdSPfTM+z/S+uXsDdwb/TlQySFnXmt9fa11+1rrdWut12XvOX73rbXOXZ/tXrNdvq9/K3svAMrM3J69H+k9f6y73NYuZ/7jJO9Ikpn5oewN3IvHusvjdzbJj++/kvptSb6+1vrK9d7UNTCzzeybcWYn5ra5vefKZ/YxvALx3iT/I3uvvPzA/nWPZO8fXbJ3p/xGkvNJ/nuSHzzqPX0HnPm/JPnfST6//3H2eu/5qM98ydpP5sZ/BfVh9/Ek+XdJnk3yh0keuN57PoYzn07y6ey9svrzSf7R9d7zBmf+WJKvJPl29h6JeCjJTyb5yQP386P7fyd/eKN/X+94P5vZZvYN+WFu3/xz+yhmtrewBgCAwjvzAQBAIZQBAKAQygAAUAhlAAAohDIAABRCGQAACqEMAADF/wMRLI+j9fxjJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "%matplotlib inline\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    #x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
    "                      layer_name = 'conv2d_32'):\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    img_ascs = list()\n",
    "    for filter_index in range(layer_output.shape[3]):\n",
    "        # build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # compute the gradient of the input picture wrt this loss\n",
    "        grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([model.input], [loss, grads])\n",
    "\n",
    "        # step size for gradient ascent\n",
    "        step = 5.\n",
    "\n",
    "        img_asc = np.array(img)\n",
    "        # run gradient ascent for 20 steps\n",
    "        for i in range(20):\n",
    "            loss_value, grads_value = iterate([img_asc])\n",
    "            img_asc += grads_value * step\n",
    "\n",
    "        img_asc = img_asc[0]\n",
    "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
    "        \n",
    "    if layer_output.shape[3] >= 35:\n",
    "        plot_x, plot_y = 6, 6\n",
    "    elif layer_output.shape[3] >= 23:\n",
    "        plot_x, plot_y = 4, 6\n",
    "    elif layer_output.shape[3] >= 11:\n",
    "        plot_x, plot_y = 2, 6\n",
    "    else:\n",
    "        plot_x, plot_y = 1, 2\n",
    "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
    "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
    "    ax[0, 0].set_title('Input image')\n",
    "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
    "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
    "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
    "        if x == 0 and y == 0:\n",
    "            continue\n",
    "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
    "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
    "\n",
    "vis_img_in_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tvptcn8dxvp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 1st DNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (Keras-FCN)",
   "language": "python",
   "name": "keras-fcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
